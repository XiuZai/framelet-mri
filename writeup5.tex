\documentclass[11pt]{amsart}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{graphicx}

\usepackage[ruled]{algorithm2e}
\SetKwInput{KwInit}{Initialize}

\usepackage{multicol}

\newcommand{\R}{\mathbb{R}}
\newcommand{\Z}{\mathbb{Z}}

\newtheorem{thm}{Theorem}[section]
\newtheorem{lem}[thm]{Lemma}

\theoremstyle{remark}
\newtheorem{rem}{Remark}
\newtheorem{defn}{Definition}


\title{TV-Framelet regularization for MRI reconstruction in the
presence of field inhomogenities.}
\author{Ryan Compton}
\date{October 26, 2010}


\begin{document}

\begin{abstract}

Rapid acquisition of MR images via reconstruction from undersampled $k$-space data has the potential to greatly decrease MRI scan time on existing medical hardware. To this end iterative image reconstrction based on the technique of compressed sensing has become the method choice for many researchers \cite{Lustig2007}. However, while conventional compressed sensing relies on random measurements from the discrete Fourier transform of our image, actual MR scans often suffer from off-resonance effects and thus generate data by way of a non-Fourier operator, complicating image reconstruction methods and introducing additional computational bottlenecks \cite{Fessler2005}.

\end{abstract}

\maketitle

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Introduction}
Our model of MR image creation requires that we solve for the proton density map, $\rho(\mathbf{r})$, in the signal equation

\begin{equation}\label{sigeq}
s(t) = \int_{\mathbb{R}^2} \rho(\mathbf{r})e^{-z(\mathbf{r})t}e^{-2\pi i \mathbf{k}(t) \cdot \mathbf{r}} d\mathbf{r} + \epsilon
\end{equation}
where $s(t)$ is the recorded signal, $z(\mathbf{r})$ contains information about relaxation and off-resonance effects, the $k$-space coordinates $\mathbf{k}(t)$ are known only on a nonuniform and possibly undersampled grid and $\epsilon$ is receiver noise. Discretizing the physical space integral in (\ref{sigeq}) at $n$ points and collecting $m$ samples in $k$-space leads to the $m \times n$ linear system

\begin{equation}\label{dissigeq}
\vec{s} = A \vec{\rho} + \vec{\epsilon}
\end{equation}
with sample matrix
\begin{equation}\label{aij}
A_{ij} = e^{-z(\mathbf{r}_j)t_i}e^{-2\pi i \mathbf{k}(t_i) \cdot \mathbf{r}_j}.
\end{equation}


A straightforward approach to this discrete reconstruction problem (\ref{dissigeq}) is infeasible for practical image sizes. For example, in order to produce a $512 \times 512$ image the (dense) $A$ matrix takes on dimensions $512^2 \times 512^2$. Storing $A$ using 32-bit floating point precision results in a matrix requiring 256GB of space, prohibitively large on most current machines. Operator compression is thus necessary before attempting to solve the linear system.

Ignoring the $z(\mathbf{r})$ factor and filling $k$-space completely on a uniform grid yields
\begin{equation}
A = \mathcal{F}
\end{equation}
the discrete Fourier matrix. We may determine $\rho$ here by simply inverting a single Fourier transform.

Failing to accomodate for off-resonance effects leads to blurring and image distortion unless the background field is perfectly homogenous while filling $k$-space completely increases scan time beyond what is needed for exact image reconstruction.

We reduce scan time by appealing to the theory of compressed sensing and downsample our Fourier matrix by multiplication with a random $m \times n$ matrix $R_{ij} = 1 \text{ if we record } k_j \text{, } 0 \text{ else}$. Thus
\begin{equation}
A = R\mathcal{F}
\end{equation}
and we account for the missing data by taking advantage of the fact that our image is sparse in some transform domain by introducing $l1$ regularization. The canonical image reconstruction problem is then
\begin{equation}\label{csequ}
\underset{\rho}{\operatorname{argmax}} \: J(\rho)  \text{ subject to } R\mathcal{F} \rho = s
\end{equation}
where $J$ represents some form of $l1$ regularization. Perfect image reconstruction is possible as $R\mathcal{F}$ satifies the restricted isometry property.

Our model follows the work of [] [] [] taking into account the phase accural due to off-resonant frequency by approximating $A$ with a combination of nonuniform Fourier transforms. That is,
\begin{equation}\label{aprox}
A = \sum_{l=1}^L B_l \mathcal{G} C_l 
\end{equation}
where $B_l$ and $C_l$ are diagonal matrices used in a rank $L$ approximation of $e^{-z(\mathbf{r})t}$ and $\mathcal{G}$ is a nonuniform discrete Fourier transform of type 2 []. While this generalized model hurts our restricted isometry constants image recovery from undersampled $k$-space is still possible when enough data is collected.

The optmization we solve is
\begin{equation}\label{gcsequ}
\underset{\rho}{\operatorname{argmax}} \: J(\rho)  \text{ subject to } A \rho = s.
\end{equation}
In this work we advocate a hybrid of total vairation and framelet regularizers,
\begin{equation}
J(\rho) = | \nabla \rho| + | D \rho |
\end{equation}
where the first term is a total vairation norm and $D$ is the discrete framelet decomposition []. As we will see, not only does this combined regularization improve image quality, it markedly decreases computational cost when compared with the standard total vairation regularizer as the subproblems we solve in our minimization become better conditioned.

\section{Method}

\subsection{Low rank inhomogeneity correction}
The factor $e^{-z(\mathbf{r}_j)t_i}$ in (\ref{aij}) is probelmatic as $t$ is variable preventing us from absorbing the exponential into $\rho$ and working with a purely Fourier system matrix. Seperable low rank approximations allow us to rapidly apply $A$ with little storage overhead. The general form is 

\begin{equation}
e^{-z(\mathbf{r})t} = \sum_{l=1}^L b_{l}(t_i)c_{l}(\mathbf{r}_j)
\end{equation}

Many methods of choosing $b_l$ and $c_l$ exist [] [] [] []. The time segmentation approach \cite{Sutton2003} selects a subset of $L$ points, $\{ \hat{t}_l \} \in [0,T]$ and interpolates about an optimal rate map value, $\overline{\omega}$,

\begin{equation}
b_{il} = b_l(t_i)e^{-\overline{\omega}t_i}
\end{equation}

\begin{equation}
c_{lj} = e^{-(z(\mathbf{r}_j) - \overline{\omega}t_i)\hat{t}_l}
\end{equation}

Application of $A$ proceeds as
\begin{equation}
(Ax)_i = \sum_{l=1} b_{il} \sum_{j=1}^n x_j c_{lj} e^{-2\pi i \mathbf{k}(t_i) \cdot \mathbf{r}_j}
\end{equation}
justifying the notation in equation (\ref{aprox}).

While fast to apply, the modified system matrix obeys uncertainty properties differing from the pure Fourier case
We recall the concept of a {\it restricted isometry constant}, $\delta_s^A$, of a matrix $A$ for an integer $s$ as the smallest number such that
\begin{equation}
(1-\delta_s^A)\|x\|_{l2}^2 \leq \|Ax\|_{l2}^2 \leq (1+\delta_s^A)\|x\|_{l2}^2
\end{equation}
where $s$ is the number of nonzero entries in $x$. When such a constant exists $A$ is said to satisfy the {\it restricted isometry propery} allowing us to make use of results from compressed sensing. In the case of a Fourier matrix $A = R\mathcal{F}$ much work has been devoted to showing that $\delta_s^{R\mathcal{F}}$ is small [candes icm].

In our corrected system matrix we can infer that each term in the sum (\ref{aprox}) has poor isometric properties by examining
\begin{eqnarray}
\|B_l R\mathcal{F} C_l x \|_2 & \leq & \|B_l\| \|R\mathcal{F} C_l x\|_2 \\ 
& \leq & \max_i(b_{il}) (1+\delta_s^{R\mathcal{F}})\max_j(c_{lj})\| x \|_2
\end{eqnarray}
as well as the corresponding lower bound to give us a restricted isometry constant for an $L=1$ correction to our inhomogeneity
\begin{equation}
\delta_{B_lAC_l} \leq \min( 1-\min_i(b_{il})\min_j(c_{lj}) + \min_i(b_{il})\min_j(c_{lj})\delta_{R\mathcal{F}}, 
1-\max_i(b_{il})\max_j(c_{lj}) + \max_i(b_{il})\max_j(c_{lj})\delta_{R\mathcal{F}}).
\end{equation}
as a result more samples are required for accurate image recovery than in the pure Fourier case.


\subsection{Frames and Framelets}

Natural images often have sparse representations within some redundant orthogonal basis $X$ for $\R^n$ []. We call such a system a ``tight frame'' if
\begin{equation}
f = \sum_{g \in X} \left< f,g \right> g.
\end{equation}
as $X$ is overcomplete the redundency often leads to improved image quality in sparse representations [].

Recall that a wavelet system is the collection of dilations and shifts of a finite set $\Psi$
\begin{equation}
X(\Psi) = \left\{ 2^{k/2}\psi(2^kx-j) \: : \: \psi \in \Psi,k,j\in \Z \right\}.
\end{equation}
When $X(\Psi)$ is a tight frame the elements, $\psi$, are called ``framelets''. Stacking the basis elements as row vectors of a matrix $D$ we have that $D^tD = I$. However $DD^t = I$ may not hold. The decomposition $Df$ may be computed efficiently via the discrete wavelet transform. We adpot the piecewise linear B-spline framelet in this work []. More details on framelets may be found in [] [] [] [] [].


\subsection{Split Bregman iterations for image recovery}

Our image is the solution to the constrained optmiziation
\begin{align}\label{jcseq}
\begin{array}{c}
\underset{\rho}{\operatorname{argmax}} \:  | \nabla \rho| + | D \rho | \\
\text{ subject to } A \rho = s
\end{array}
\end{align}
which we solve with the Split Bregman method of Goldstein et al. []. We begin by converting the constrained optimization (\ref{jcseq}) into a sequence of unconstrained problems via Bregman iteration []
\begin{align}\label{gbregman}
\left\{
    \begin{array}{rcl}
\rho^{k+1} &=& \min_\rho | \nabla \rho| + | D \rho | + \frac{\mu}{2} \| A\rho - s^k \|^2 \\
s^{k+1} &=& s^k + s - A\rho^{k+1}
\end{array}
\right.
\end{align}
where the parameter $\mu$ affects the convergence rate and is chosen by the user. Alternatly updating $\rho^k$ and $s^k$ produces a sequence $\rho^k \rightarrow \rho$, the solution to the constrained problem.

Updating $s^k$ is straightforward. Minimization of the unconstrained step in (\ref{gbregman}) is done by introducing auxillary variables, $d_x = \nabla_x \rho$, $d_y = \nabla_y \rho$, and $w = D\rho$, allowing us to rewrite our $\rho^k$ update in the equlivalent split form

\begin{align}\label{spilt}
    \rho^{k+1} & \left.\begin{array}{l} = \min_\rho \|(d_x,d_y)\|_2 + | w | + \frac{\mu}{2} \| A\rho - s^k \|^2 \\ \end{array}\right. \\
        & 
\text{subject to} \left\{ \begin{array}{l}
       d_x = \nabla_x \rho  \\
       d_y = \nabla_y \rho  \\
       w = D \rho
    \end{array}\right. 
\end{align}

This constrained optimization is then converted to a sequence of unconstrained problems via a second Bregman iteration. This leads us to the following algorithm

\begin{algorithm}[H]
\caption{Split Bregman iteration for constrained optimization}
\label{sbalgo}

\KwInit{ $\rho^0 = A^t s$, and $d_x^0=d_y^0=w^0=b_x^0=b_y^0=b_w^0=0$}

\While{ $\| A\rho^k - s \|_2^2 > tol$ }{
\For{$i=1 \text{ to } n_{inner}$}{

$\rho^{k+1} = \min_\rho \left\{ \begin{array}{l} \frac{\mu}{2}\|A\rho - s^k\|^2 + \frac{\lambda}{2}\|d_x^k - \nabla_x\rho - b_x^k\|^2 \\ + \frac{\lambda}{2}\|d_x^k - \nabla_x\rho - b_x^k\|^2 + \frac{\gamma}{2}\|w^k - D\rho - b_w^k\|^2 \end{array} \right\}$\;

$d_x^{k+1} = shrink(\nabla_x \rho^{k+1} + b_x^k, 1/\lambda)$\;
$d_y^{k+1} = shrink(\nabla_y \rho^{k+1} + b_y^k, 1/\lambda)$\;
$w^{k+1} = shrink(D \rho^{k+1} + b_w^k, 1/\gamma)$\;
$b_x^{k+1} = b_x^{k} + (\nabla_x \rho^{k+1} - d_x^{k+1})$\;
$b_y^{k+1} = b_y^{k} + (\nabla_y \rho^{k+1} - d_y^{k+1})$\;
$b_w^{k+1} = b_w^{k} + (D \rho^{k+1} - w^{k+1})$\;

}
$s^{k+1} = s^k + s - A\rho^{k+1}$\;
}
\end{algorithm}


Here, the function $shrink$ comes from the wavelet literature[] and is defined as
\begin{equation}
shrink(x,a) = \frac{x}{|x|} \max (|x|-a,0).
\end{equation}
The constants $\lambda$ and $\gamma$ are chosen by the user and affect the convergence rate.

Computationally, the $\rho^{k+1}$ update is the most expensive part of our algorithm by a wide margin. The speed of our image reconstruction is thus largely determined by how fast we can solve this minimization. In the purely Fourier case an analytic solution exists leading to a notably fast algorithm []. We have no such formula for the generalized $A$ we work with and instead rely on iterations of the conjugate gradient method to update $\rho^{k+1}$.

By differentiating with respect to $\rho$ and setting the result to zero we find our $\rho^{k+1}$ update as the solution to
\begin{equation}
(\mu A^tA + \lambda \nabla_x^t \nabla_x + \lambda + \gamma D^tD )\rho^{k+1} = rhs^k
\end{equation}
where
\begin{equation}
rhs^k = \mu A^t s^k + \lambda \nabla_x ^t (d_x^k - b_x) + \lambda \nabla_y^t + \gamma D^t(w - b_w).
\end{equation}
Making use of the identities $\nabla^t \nabla = - \triangle$ and $D^tD = I$, gives the system
\begin{equation}\label{hybridsys}
(\mu A^tA - \lambda \triangle + \gamma I )\rho^{k+1} = rhs^k
\end{equation}
which we solve with conjugate gradient iterations.

A major advantage of our hybrid regularizer is now apparent. Consider the system resulting from a regularization based only on total variation (ie $\gamma =0$),
\begin{equation}\label{tvonly}
(\mu A^tA - \lambda \triangle)\rho^{k+1} = rhs^k.
\end{equation}
With $\lambda_{\max}$ and $\lambda_{\min}$ denoting maximal and minimal eigenvalues we can write the condition number of the matrix in (\ref{tvonly}) as
\begin{equation}
\kappa = \frac{\lambda_{\max}}{\lambda_{\min}}.
\end{equation}

The addition of the framelet regularizer introduces the $\gamma I$ term in (\ref{hybridsys}) reducing the condition number to
\begin{equation}
\kappa_\gamma  =  \frac{\lambda_{\max} + \gamma}{\lambda_{\min} + \gamma} < \kappa
\end{equation}
notably speeding our updates.

\section{Numerical Results}

Our field corrected reconstruction method is compared against three alternate approaches: standard nonuniform fft, $l2$ regularized CG iterations [], and total vairation regularization [].





\bibliographystyle{plain}
\bibliography{thebib}

\end{document}
